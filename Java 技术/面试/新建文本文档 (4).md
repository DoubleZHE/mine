# 腾讯面试



## Java

1. ArrayList  和 LinkedList 的区别

   * ArrayList 的底层实现是数组，LinkedList底层是链表
   * ArrayList 实现了 List 接口。ArrayList 按索引查询很快；不指定索引插入默认是插入到数组的末端，如果指定索引插入的话，需要进行元素的移动，效率没有链表好（删除元素也需要移动元素）。并且ArrayList 的添加元素可能会需要扩容；**如果合理设置ArrayList 的容量，并且采用尾插法，ArrayList 的插入效率不比LinkedList 差。**
   * LinkedList 不仅实现了 List 接口，还实现了 Deque 接口，LinkedList 中有两个指针分别指向链表的头结点和尾结点，在按索引插入元素时，如果不指定索引默认是尾插，只需要常数时间，如果指定索引且索引不等于size 时，会先根据 index < (size >> 1) 来判断往哪边插入（按索引查询同理）。**删除元素指的是删除第一次出现的这个元素，由于LinkedList 可以存放null元素，故也可以删除第一次出现null的元素**。**clear() 方法为了GC更快，会遍历节点，把节点之间的引用关系赋空**。**遍历LinkedList 不要使用随机遍历，通常使用foreach 遍历。**
   * ArrayList 适合随机查找，LinkedList 适合删除和添加。LinkedList 可以作为双端队列使用

2. 说一下 HashMap 的 put 方法

   * **首先Key 通过 哈希算法计算哈希值**（h ^ (h>>16)，结合高低位的特征，降低哈希碰撞），**然后和数组长度进行与运算**（h & （length - 1），之所以和数组长度-1进行与运算，一个原因是可以防止计算出的索引越界，另一个原因，也是为什么2倍扩容的原因，length -1 的二进制是 最左位是0，其余位全是1，这样低位任何一位的区别都会让结果索引不同，而且对应的索引位只有一种哈希值的低位与之对应，另一个好处是在扩容的时候，需要重新计算索引，2倍扩容的话，只要 h 对应的最左边的那一位的差异位是0，就不需要动之前老数组的索引，减少了之前已经散列好的数据重新调换）
   * 如果数组下标位置为空，则将 key 和 value 封装为Entry 对象（jdk1.7 是Entry，1.8 是Node对象），并放入该位置
   * 如果数组下标不为空，就分情况讨论
     * 如果是JDK1.7，**先判断是否需要扩容**，要扩容就先扩容，如果不用就用**头插法**添加到当前位置的链表中（头插法在**多线程**的情况下可能会出现**环形链**，因为在扩容之后，会重新计算每个元素的位置，各个元素之前的引用也会改变，所以多线程下可能会出现环形链。jdk1.8 改用尾插法，元素之前的引用顺序不会改变，也就不会有环形链）
     * 如果是JDK1.8，会先判断当前位置上的 Node 的类型，看是红黑树Node，还是链表Node
       * 如果是红黑树Node，则将key和value封装为红黑树节点并添加到红黑树中，这个过程中会判断红黑树中是否存在当前key，如果存在则更新value
       * 如果是链表Node，则将key和value封装为链表Node，并遍历链表，如果遍历中发现存在当前key，则更新value，不存在用尾插法插入。插入后，看当前链表节点个数，如果超过8，则链表转红黑树
       * 不管哪种情况，插入完成后，判断是否要扩容
   * 第一次put 操作时才真正构建 table 数组，如果key 为null，存储位置为 table[0] 或 table[0] 的冲突链上

3. HashTable

   * HashTable 也实现了 Map 接口，但与HashMap 不同的是，HashTable 继承的是 Dictionary 类，HashMap 继承的是 AbstractHashMap 类。HashTable 继承了Dictionary 的一个 elements 方法，返回的是所有值的枚举。
   * HashTable 不允许 key 为 Null ，会抛出空指针异常
   * HashTable 计算key 的hash 值是用的Object 的 hashCode 方法，在计算索引的时候，用的公式是 （hash & 0x7fffffff）% tab.length ，0x7fffffff 是 第一位是0，其余31位都是1 的二进制数，然后和 tab 的长度取模获取索引位置，取模法比HashMap 的位运算法效率低
   * HashTable 的 扩容机制是 2*N + 1，使得数组的长度尽量为奇数或素数，这样可以减少 Hash 碰撞。
   * HashTable 的方法都有被 Synchronized 修饰。HashTable 在扩容时，采用的头插法迁移数据，使用头插法的效率高，虽然 rehash() 方法没有被 Synchronized 修饰，但是所有调用 rehash 方法的方法均被 Synchronized修饰

4. ConcurrentHashMap

   * JDK1.7 中，采用 分段锁 **Segment + HashEntry**，HashMap 是只有一个数组来存储，而ConcurrentHashMap 是把 这样的数组拆分成多个，每一个分组配一把锁，当一个线程占用锁访问其中一段数据，其他段的数据也能被其他线程访问到。
   * Segment 继承了 ReentrantLock（可重入锁），默认有16 个Segment ，也就是并发度默认是16 ；内部有个 HashEntry[] 数组，HashEntry 类相当于 HashMap 中的 Node 节点，不过**HashEntry 中的 value 和 next 都被 volatile 修饰，保证多线程下数据获取时的可见性。**
   * 在进行 put 时，会先hash定位到 是哪个Segment，在定位元素所在的链表头部**（二次定位）**。首先会尝试获取锁，**如果获取失败，就通过 scanAndLockForPut() 方法自旋获取锁，如果重试次数达到 Max_Scan_Retries 改为阻塞锁获取，保证能获取成功**。
   * get 时，也是二次定位，由于HashEntry 的共享变量被 volatile 修饰，所以每次获取到的都是最新值。**get 不需要加锁，因为 volatile 保证了可见性**
   * JDK1.8 改用了 和jdk1.8中 HashMap相同的 **数组+链表+红黑树**的结构，采用 **CAS + synchronized 锁住链表的头结点**，实现了更小的锁粒度。JDK1.8 **之所以采用 synchronized 替换 可重入锁** ReentrantLock ，是因为**JDK1.6 对 synchronized 锁的实现进行了大量优化，并且 synchronized 有多种锁状态，会从无锁 -> 偏向锁 -> 轻量级锁 -> 重量级锁一步步转换**。Node 节点的 value 和 next 都用了 volatile 修饰，保证可见性
   * put 时，根据 key 计算hash值，然后判断是否需要初始化 table，然后定位到 Node，拿到首节点，判断首节点：
     1. 如果为 null，通过 CAS 尝试添加
     2. 如果 **Node.hash = MOVED = -1**，说明其他线程在扩容，参与一起扩容
     3. 如果都不满足，synchronized 锁住Node 节点，判断是链表还是红黑树，遍历插入，链表大于8时扩容或者转换成红黑树
   * get 无须加锁，因为volatile保证了可见性
   * ConcurrentHashMap **不支持 key 或 value 为 null**。value不能为null 的原因是，ConcurrentHashMap 用于多线程，如果ConcurrentHashMap .get(key) 返回 null，不知道这个null 是没有映射的value 是 null，还是 没找到对应的key 而为null，**有二义性**。而 key 不能为 null 源码就是这么写的，设计之初就不允许key 为 null
   * ConcurrentHashMap **迭代器是弱一致性**。迭代器创建后，就会按照哈希表结构遍历每个元素，但在遍历过程中，内部元素可能会发生变化，**如果变化发生在已遍历过的部分，迭代器就不会反映出来，而如果变化发生在未遍历过的部分，迭代器就会发现并反映出来**，这就是弱一致性

5. ThreadLocal

   * ThreadLocal 叫做线程变量，意思是ThreadLocal 中填充的变量属于当前线程。ThreadLocal 变量通常被 **private static** 修饰。当一个线程结束时，它所使用的所有ThreadLocal 相对的实例副本都可被回收。**适用于每个线程需要自己独立的实例且该实例需要在多个方法中被使用，也就是说变量在线程间隔离，在方法或类间共享的场景**

   * 简单使用

     ![](https://gitee.com/swlfox/picture-bed/raw/master/Img/image-20230320234355269.png)

   * ThreadLocal 的 set 方法

     * ThreadLocal set 赋值时，首先获取当前线程thread，并获取 **thread 的 ThreadLocalMap 属性**，如果map属性不为空，则直接更新value，如果为空，先实例化ThreadLocalMap，再将value 值初始化。**map.set(threadLocal, value)**
     * ThreadLocalMap 使用静态内部类 Entry 保存数据，**Entry 继承的弱引用（WeakReference）**，Entry 中，ThreadLocal 作为 key，value 作为 value。
     * ThreadLocal 的get 和 set 方法都是先获取当前 thread 对象，然后获取 thread 的 ThreadLocalMap 属性，再通过自身 作为 key 去 map 中查找 value，或更新value

   * ThreadLocal 的 remove 方法

     * remove 方法是从当前的thread 的 ThreadLocalMap 中删除该 ThreadLocal 对应的值。删除的原因涉及到**内存泄漏**
     * ThreadLocalMap 中使用的 key 为 ThreadLocal 的**弱引用**，如果这个对象只存在弱引用，那么下一次垃圾回收必然会被清理掉。
     * 所以如果ThreadLocal 没有被外部强引用的情况下，会被GC清理掉，这样的话 ThreadLocalMap 中使用的这个 ThreadLocal 的 key 也就被清理掉了，但是，**value 是强引用，不会被清理，这样就会出现 key 为 null 的 value**
     * ThreadLocal 是与线程绑定的变量，这样就会出现一个问题：**如果不将 ThreadLocal 对应的 value 删除或替换，该 value 就会和 Thread 共存**。通常**线程池会采用线程复用**，在线程池中的线程很难结束甚至永不结束，与JVM生命周期一致。这样就会导致内存泄漏
     * 应用场景：连接管理，一个线程持有一个连接，该连接可以在不同方法之间进行传递，线程之间不共享同一个连接

6. List 和 Set 的区别

   * List ： 有序，按对象进入的顺序保存对象，可重复，允许多个 null 元素对象，可以使用 迭代器取出所有元素逐一遍历。还可以根据索引获取指定的元素
   * Set ： 无序，不可重复，最多只允许一个 Null 对象，取元素只能用迭代器取得所有元素逐一遍历

7. 线程池（**ThreadPoolExecutor**）的核心参数

   * **corePoolSize** **线程池核心线程大小**：是线程池中的**最小的线程数量**，即使这些线程处于空闲状态，也不会被销毁，除非设置了allowCoreThreadTimeOut
   * **maximumPoolSize** **线程池最大线程数量**：一个任务被提交到线程池以后，首先会找有没有空闲并且存活线程，如果有就直接把任务给空闲线程执行。如果没有，则会放到工作队列中，**当工作队列满了，才创建一个新线程**，然后从工作队列头部取出一个任务给新线程。线程池不会无限创建新线程，有个最大线程数量限制，**工作队列满，且线程数等于最大线程数，此时再提交任务则会调用拒绝策略**
   * **keepAliveTime 多余的空闲线程存活时间**：**当线程池的线程数大于 corePoolSize 时**，如果有多余的空闲线程的空闲时间达到keepAliveTime 时，多余的线程会被销毁
   * **unit 空闲线程存活时间单位**：keepAliveTime的计量单位，如**TimeUnit.MILLISECONDS**（毫秒）、**TimeUnit.SECONDS**（秒）、**TimeUnit.MINUTES**（分）
   * **workQueue 工作队列**：任务被提交给线程池时，会先进入工作队列，任务调度时再从工作队列中取出
     1. **ArrayBlockingQueue**（数组的有界阻塞队列）：创建时必须设置大小，按先进先出排序。
     2. **LinkedBlockingQueue**（链表的无界阻塞队列）：可以设置容量（有界队列），不设置容量默认使用 Integer.Max_Value ，此时参数 maxPoolSize 不起作用了。Executors.**newSingleThreadExecutor**、Executors.**newFixedThreadPool**，使用了这个队列
     3. **SynchronousQueue**（不缓存任务的阻塞队列）：新任务进来，不会缓存，而是直接被调度执行该任务，如果没有可用线程，则创建新的线程，达到maxPoolSize，执行拒绝策略。Executors.**newCachedThreadPool** 所创建的线程池使用此队列
     4. PriorityBlockingQueue (具有优先级的无界阻塞队列)：优先级通过参数 Comparator 实现
     5. **DelayQueue** （无界阻塞延迟队列）：**底层是 PriorityBlockingQueue** ，**队列中每个元素都有过期时间**，当从队列获取元素时，只有已经过期的元素才会出队。快捷工厂方法 Executors.**newScheduledThreadPool** 所创建的线程池使用此队列
   * 阻塞队列和普通队列相比，在阻塞队列为空时，会阻塞当前线程的元素获取操作，在一个线程从空的阻塞队列中取元素时，线程会被阻塞，直到阻塞队列中有元素；当队列中有元素时，被阻塞的线程自动被唤醒
   * **threadFactory 线程工厂**：用来创建线程，可用设定线程名等
   * **handler 拒绝策略**：
     * AbortPolicy：丢弃任务并抛出异常 （默认）RejectedExecutionException
     * DiscardPolicy：丢弃任务，不抛异常
     * DiscardOldestPolicy：丢弃队列最前面的任务，把新任务加入到队尾
     * CallerRunsPolicy：谁调用，谁处理。由调用线程（即提交任务给线程池的线程）处理该任务，如果线程池没了，就直接丢弃
   * 关闭线程池
     * shutdown() : 设置线程池的状态为 shutdown，然后中断所有**没有正在执行的任务**的线程
     * shutdownNow() : 设置线程的状态为 stop，然后尝试停止所有正在执行或暂停任务的线程，并返回等待执行的任务列表
   * 功能线程池
     * 定长线程池 FixedThreadPool
       * 只有核心线程，线程量固定，执行完立即回收，任务队列为链表有界队列
     * 定时线程池 ScheduledThreadPool
       * 核心线程固定，非核心线程无限，执行完闲置10ms回收，任务队列为延时阻塞队列
     * 可缓存线程池 CachedThreadPool
       * 无核心线程，非核心线程无限，执行完闲置60s 后回收，任务队列为 不存储元素的阻塞队列。执行大量、耗时少的任务
     * 单线程化线程池 SingleThreadExecutor：
       * 一个核心线程，无非核心线程，执行完立即回收，任务队列为链表结构有界队列

8. Java 死锁如何避免

   * 造成死锁的原因
     1. 互斥条件：一个资源只能被一个线程占用，当这个资源被占用后其他线程只能等待
     2. 不可剥夺条件：当一个线程不主动释放资源时，此资源一直被拥有线程占有
     3. 请求与保持条件：线程在请求新的资源时，已有的资源不释放
     4. 环路等待条件：产生死锁一定是循环等待资源
   * 前三个条件是作为锁要符合的条件，所以避免死锁需要不出现循环等待锁的
     1. 注意加锁顺序，保证每个线程按同样的顺序进行加锁
     2. 注意加锁时限，可以针对锁设置一个超时时间
     3. 注意死锁检查，这是一种预防机制，确保在第一时间发现死锁并进行解决





## Spring 

1. Bean 是线程安全的吗
   * Spring 本身没有针对 Bean 做线程安全的处理，所以：如果Bean 是无状态的，那么Bean 是线程安全的，如果有状态，则不是线程安全的。Bean 是不是线程安全和作用域没有关系。











## JVM

1. JVM 中 哪些是共享区，哪些可以作为 GC Roots
   * 堆 和 方法区 是共享区，虚拟机栈、本地方法栈、程序计数器线程独享
   * JVM 在进行垃圾回收时，需要寻找垃圾对象，是采用 **可达性分析法** 来判断对象是否可以回收，可达性分析法是从 GC Roots 开始向下搜索，搜索所走过的路程为引用链，如果一个对象到GC Roots 没有任何引用链，则该对象是不可用的，可以被回收。
   * 因此 能作为 GC Roots 的有一个特征，它只会引用其他对象，而不会被其他对象引用，例如：**栈中的本地变量、方法区的静态变量、本地方法栈中的变量、正在运行的线程 等可以作为 GC Roots**
2. 项目如何排除JVM 问题
   * 对于还在正常运行的系统：
     1. 可以使用 jmap 查看JVM 中各个区域的使用i情况
     2. 可以通过 jstack 查看线程的运行情况，比如哪个线程阻塞、是否出现了死锁
     3. 可以通过 jstat 查看垃圾回收情况，特别是 full GC，如果发现 full GC 比较频繁，就需要进行调优了
     4. 通过各个命令的结果，或 Jvisualvm 等工具进行分析
     5. 首先，初步猜测是 频繁 full GC，如果频繁 full GC 但是又一直没有出现内存溢出，那表示 full GC 实际上回收了很多的对象，这些对象最好能在 young 过程中被回收掉，避免这些对象进入 老年代。对于这种情况，就要考虑这些存活时间不长的对象是不是比较大，导致年轻代放不下，直接进入了老年代，尝试加大年轻代的 大小（**-Xmn** 16M），如果改完后，full GC 减少，证明修改有效
     6. 同时，还可以找到占用CPU最多的线程，定位到具体的方法，优化这个方法的执行，看是否能避免某些对象的创建，从而节省内存
   * 对于已经发生 OOM 的系统：
     * 一般生产系统会设置当 OOM 时，生成当时的 dump 文件（-XX: +HeapDumpOnOOMError -XX:HeapDumpPath=...）
     * 可以利用 jvisualvm 等工具分析 dump 文件
     * 根据 dump 文件 找到异常的实例对象，和异常的线程（CPU占用高），定位到具体的代码
     * 然后再进行详细的分析和调试





## MySQL

1. 有没有关心系统的sql 耗时？统计过慢查询吗？对慢查询怎么优化

   * 慢查询的优化首先要弄明白慢的原因：查询条件没有命中索引、加载了不需要的数据列、数据量太大
   * 针对三个方向来
     * 首先分析语句，是否加载了额外的列，可能是查询了多余的行并且抛弃掉了，可能是加载了许多结果中不需要的列。对语句进行分析并重写
     * 分析语句的执行计划（Explain + 语句），然后获取其索引的使用情况，之后修改语句或修改索引，使得语句尽可能的命中索引
     * 如果对语句的优化已经无法进行，可以考虑表中的数据量是否太大，是的话可以进行分表

2. 索引失效的常见原因

   * 常见原因：

     1. **不满足最左前缀原则**

     2. **范围索引列没有放到最后**：**注意，范围查询放最后，是指联合索引中的列，把范围查询的那个列放到最后**。例如，设置联合索引为 （code，age，name），where code = ‘001’ and age > 18 and name = '张三'，你怎么调整 where 子句中的三个条件都没有，联合索引因为 中间的 age 是范围查询断掉了。这个时候如果把联合索引 改为 （code，name，age），也就是把 age 放到最后，这个时候就能完全利用联合索引了。

     3. **使用了 Select *** : 使用 select * 走的是全表扫描，如果 select 索引列，就是走的全索引扫描，效率更高。这里用到了 **覆盖索引，如果 select 的列不只包含索引列，则需要回表**

     4. **索引列上有计算或加了函数**：索引列上有计算或索引列上加了函数，索引会失效

     5. **字符类型没加引号**：类型不匹配导致索引丢失，**使用字符类型字段做判断时，一定要加上单引号**

     6. **字段不允许为空的时候使用 is null 或 is not null** ：这种情况索引会失效。如果字段允许为空，is null 走 ref 类型的索引，is not null 走 range 类型索引

     7. **like 查询左边有 %** ：左边有 % 索引会失效，右边有%，走的range 类型的索引。如果非要使用like 左边有%，使用 覆盖索引 的方式，如:

        ~~~mysql
        select * from test1 where code like '%001%';
        -- 修改为，(code,age,name 联合索引)
        select code, age, name from test1 where code like '%001%';
        ~~~

     8. **使用 or 关键字时没注意**：

        ~~~mysql
        -- height 是索引，走的 range 类型索引
        select * from test1 where height = 8 or height = 9;
        -- 全表扫描，索引失效。如果单独查询，都走 ref 索引
        select * from test1 where code = '001' or height = 8;
        -- 如果单独查询，都走 ref 索引
        select * from test1 where code = '001';
        
        -- 使用 union 解决需要使用 or 的场景
        (select * from test1 where code = '001') union (select * from test1 where height = 8)
        ~~~

   * 索引设计建议

     * 优先使用唯一索引，能快速定位
     * 为常用查询字段建索引
     * 为排序、分组和联合查询字段建索引
     * 一张表的索引不超过5个
     * 表数据量少，可以不用建索引
     * 尽量使用占用空间小的字段建索引
     * 删除没用的索引，因为它会占一定空间
     * 用 idx_ 或 unx_ 等前缀命名索引，方便查找



## Redis 

1. RDB 和 AOF

   * RDB

     * RDB 是在指定的时间间隔内将内存中的数据集**快照**写入磁盘中。会创建一个子进程，然后将数据集写入到临时二进制文件，写入成功后，在替换之前的文件，然后子进程退出。触发机制如下：

       ![image-20230311191711227](https://gitee.com/swlfox/picture-bed/raw/master/Img/image-20230311191711227.png)

     * 优点：

       * 整个Redis 只有一个 dump.rdb，方便备份，很容易将 rdb 文件移动到其他的存储物质上。
       * RDB 可以最大化 Redis 的性能：父进程在保存 RDB 文件时只要创建一个子进程，然后由子进程来处理所有保存工作，父进程不需要执行任何磁盘 I/O 操作。
       * 数据集很大时，比AOF 的启动效率更高

     * 缺点：

       * 数据安全性低，RDB 是间隔一段时间进行持久化，如果持久化之间 redis 发生故障，会有数据丢失，RDB 适用于对数据完整性要求不是很严格的情况。
       * 由于RDB 是通过 创建子进程来协助完成持久化工作，所以，当数据据较大时，可能会导致整个服务器停止服务一段时间（几百毫秒，甚至1秒）

   * AOF

     * 采用**日志**的形式来记录每个写操作，追加到文件中，通过 **append 模式**写文件，即使中途服务器宕机，也不会破坏已经存在的内容，可以通过 redis-check-aof 工具修复。因为采用追加方法，如果不做任何处理的话，AOF 文件会变得越来越大，redis 提供了 **重写机制**，当AOF 大小超过阈值时，redis 会对AOF 进行内容压缩，只保留可以恢复数据的最小指令集。
     * 优点：
       * 数据的完整性和一致性更高
     * 缺点：AOF 记录的内容多，文件越来越大，数据恢复也越来越慢

2. Redis 的数据结构

   * Redis 的数据结构是针对 value 来的：
     1. 字符串：可以用来做最简单数据缓存，可以缓存简单的字符串，Json格式的字符串等，Redis 分布式锁的实现就是利用了这种数据结构，还包括实现计数器、Session共享、分布式ID
     2. 哈希表：用来存储 key-value 键值对，适合用来存储对象
     3. 列表：Redis 列表可以通过命令的组合，既可以当作栈，也可以做为 队列来使用，可以用来缓存类似微信公众号、微博等消息流数据
     4. 集合：可存储多个元素，但是不能重复，集合可以进行交集、并集、差集操作，从而实现类似我和某人的共同关注的人、朋友圈点赞等功能
     5. 有序集合：可以设置顺序，可以用来实现排行榜功能
   * set 和 Zset 的区别
     * set 相当于 Java 中的 HashSet，它内部的键值对无序唯一，内部实现相当于一个特殊的字典，字典中所有的value都是一个值null。适用于需要去重的场景
     * Zset 一方面是一个set，保证value 唯一，另一方面给 value 赋予了一个 score，代表这个value的排序权重，内部实现用的**跳跃列表**
     * 因为Zset 使用的是链表结构，这个链表按照 score 值进行排序，但插入元素时，无法通过二分查找来找到插入点。所以使用跳跃列表的方法
     * 跳跃链表类似于 学生-> 组长 -> 班长 这样的层级制，最下面一层的所有元素串起来，然后每隔几个元素挑选一个代表，代表用另一级指针串起来，在在这级里挑选新的代表，形成金字塔结构。定点插入时，先在顶层进行定位，然后下潜到下一级定位，一直下潜到最底层找到合适位置，将元素插入进去。跳跃链表采取一个随机策略来决定新元素可以兼职到第几层

3. Redis 为什么这么快

   1. 完全基于内存，数据存在内存中，绝大部分请求是纯粹的内存操作，非常快，避免了通过磁盘I/O读取到内存这部分开销
   2. 数据结构简单，对数据的操作也简单。Redis 的数据结构是专门进行设计的，每种数据结构都有一种或多种数据结构来支持。
   3. 采用单线程，省去了很多的上下文切换的时间和CPU消耗，不用去考虑锁的问题
   4. 构建了自己的VM机制，避免调用系统函数时，浪费时间去移动和请求
   5. 使用基于 IO 多路复用机制的线程模型，可以处理并发的连接

4. Redis 为什么是单线程

   * 这里的单线程是指，网络请求模块使用一个线程来处理，即一个线程处理所有网络请求，其他模块还是用的多线程
   * Redis 6 之前不用多线程是因为使用单线程可维护性高，不用去考虑多线程带来的并发读写等问题
   * Redis 6 之后引入多线程是用来处理 网络IO 这部分的，充分利用CPU 资源，减少网络IO阻塞带来的性能损耗
   * 如何开启多线程：conf 文件中，io-threads-do-reads yes 
   * 多线程下是否存在线程并发安全问题：Redis 的多线程只是用来处理网络数据的读写和协议的解析，执行命令仍然是单线程顺序执行的，不存在并发安全问题

5. Redis 除了缓存还有其他什么用法

   1. 最新列表：如新闻列表页面的最新的新闻列表，尝试用 redis 的 LPush 命令构建 List，一个个顺序塞进去就可以了。如果万一内存清理掉了，简单，查询不到存储key的话，用mysql 查询并且初始化一个List 到 redis就好了
   2. 排行榜：使用Zset
   3. 计数器，记录用户在一年中每天的点击量，只要将用户ID以及相关的日期作为键，并在每次用户点击页面时，执行一次自增操作即可
   4. 数据排重：使用 Set



## TCP/IP

1. 七层模型（应表会传网链物）
   * **应用层**：访问网络服务的接口，协议：FTP、**HTTP**、**DNS**、SNMP、Telnet
   * **表示层**：提供数据格式转换服务，协议：URL加密、口令加密
   * **会话层**：建立端连接并提供访问验证和会话管理（**Session**）
   * **传输层**：提供应用进程之间的逻辑通信，协议：**TCP、UDP**、进程、端口。数据：**Segment（数据段）**
   * **网络层**：为数据在节点之间传输创建逻辑链路，并分组转发数据。协议：**IP**、RIP、OSPF。数据：**Packet（数据包，分组）**
   * **链路层**：在通信的实体间建立数据链路连接，网卡、网桥、数据：**帧**
   * **物理层**：为数据端设备提供原始比特流的传输通路，数据：**比特**
2. TCP 和 UDP 的区别
   1. TCP ：传输控制协议，面向连接的（通过三次握手建立连接，四次挥手解除连接）、可靠的（超时重传、数据校验等方式来确保数据无差错，不丢失，不重复，按序到达）、基于字节流。逻辑通道信道是全双工可靠信道。使用场景（比如付费、加密数据等）
   2. UDP ：用户数据报协议，无需建立连接（发送数据之前不需要建立连接，带来了高效的传输效率，但是无法确保数据发送成功）、不保证可靠交付（可能出现丢失、重复）、面向报文、不可靠信道。使用场景主要有广播通信、视频、音频等多媒体通信（即时通信，QQ用的UDP协议）
3. 三次握手 和 四次挥手
   * 三次握手：在建立 TCP 连接时，需要三次握手
     * 步骤：
       1. 服务端新建套接字（套接字=点分十进制IP地址：端口号），绑定地址信息后开始监听，进入**Listen状态**。
       2. 客户端新建套接字绑定地址信息后调用 **connect**，发送连接请求 **SYN**，并进入 **SYN_SENT 状态**，等待服务器的确定（第一次握手，**客户端向服务端证明，我有数据发送能力**）
       3. 服务端一旦监听到连接请求，就会将连接放入内核等待队列，并向客户端发送 SYN 和 确认报文段ACK，进入 SYN_RECD 状态（第二次握手，**服务端向客户端证明，我有数据发送和接收能力**。**客户端也得出自己的数据发送能力正常**）
       4. 客户端收到 SYN + ACK 报文后，向服务端发送确认报文ACK，进入 ESTABLISHED状态，开始读写数据。（第三次握手，**客户端向服务端证明，我有数据接收能力，服务端得出自己数据发送能力正常**）
       5. 服务端一旦收到客户端的确认报文，进入 establish 状态，开始读写数据。
     * 为什么三次握手，而不是两次或四次
       * tcp 通信需要确保双方都具备数据接收和发送能力，三次握手刚刚好，两次不安全，四次没必要
     * 三次握手可以携带数据吗？
       * 前两次不可以携带，如果第一次可以携带数据，有人恶意攻击服务器，每次都在第一次握手中的SYN报文放入大量数据，重复发送大量SYN，服务端会花费大量内存空间缓存这些报文，服务端更容易被攻击
     * 三次握手失败，服务端如何处理
       * 两种情况，一种是服务端没有收到 SYN，则什么都不做；另一种是服务端回复了 SYN + ACK，长时间没有收到 ACK，则超时后就会发送RST 重置连接报文，释放资源
   * 四次挥手
     * 步骤
       1. 客户端主动**调用 close** 时，向服务端发送结束报文段 **FIN**，同时进入 **FIN_wait1状态**
       2. 服务端收到 FIN 时，返回确认报文 **ACK** 并进入 **Close_wait** 状态，此**时如果服务器端有数据发送的话，客户端依然需要接收**。客户端接**收到 ACK 后**，进入 **FIN_wait2 状态**，等待服务器的结束报文段
       3. 服务端数据发送完毕后，当**服务器真正调用 close** 关闭连接时，会向客户端发送 **FIN**，此时服务器进入 **Last_ACK 状态，等待最后一个 ACK 的到来**
       4. 客户端接收到 **FIN 后**，进入 **Time_wait 状态**，并**发送 ACK 给服务端**。服务端收到 ACK 后，进入 Closed 状态，断开连接。客户端需要**等待 2MSL 时间**，才进入Closed 状态
     * 为什么握手三次，挥手四次
       * 其实第二次握手时，接收端 SYN 和 ACK 合并到一个包中发送的，所以减少了一次包的发送。
       * 对于四次挥手，因为 TCP 是 全双工通信，主动关闭方发送 FIN 不代表完全断开连接，只能表示 主动方不再发送数据了。而接收方可能还要发送数据，就不能立即关闭服务端到客户端的数据通道，所以不能 FIN 和 ACK 一起发，只能先发 ACK，等服务器不需要发送 数据时，再发送 FIN。
     * Time_wait 状态有什么用，为什么不直接进入 Closed 状态释放资源
       * 如果直接进入 Closed 状态，被关闭方发送FIN后，没有得到ACK确认，超时后就会重新发送一个FIN。下次启动新的客户端有可能使用与之前客户端一样的地址信息，会有两个危害：一是这个刚启动的新客户端绑定地址成功时，就收到了一个重传的 FIN，对新连接就会造成影响。二是，如果新客户端向相同的服务端发送 SYN 请求，但是此时服务端处于 Last_ACK 状态，要求收到 ACK，而不是SYN ，因此就会发送 RST 重新建立请求。
     * 为什么 Time_wait 需要等待 2MSL 才能进入 Closed 状态
       * MSL 指报文在网络中最大生存时间。在客户端发送对服务端的FIN 确认包 ACK 后，这个ACK 可能到达不了，服务端如果接收不到ACK，就会重新发送 FIN。所以客户端需要预留 2MSL 时间（ACK 到达服务器时间 + 服务器再次发送 FIN 时间）等待确认服务器确实收到了 ACK。如果2MSL 时间后没有收到 服务端重传的 FIN，说明 服务端已经收到了 ACK。
     * 出现大量 Time_wait 
       * 说明这台主机上发起大量的主动关闭连接，应该调整 time_wait 的等待时间，或开启套接字地址重用选项
     * 出现大量 close_wait 
       * 有可能是被动关闭方主程序忘了最后一步断开连接后调用 close 释放资源，只需要加上对应的 close 即可解决问题
     * 保活机制
       * 如果两端长时间没数据来往，这时每隔一段时间，服务端向客户端发送一个保活探测数据报，要求客户端进行回复，如果多次没收到回应，就认为连接已经断开。长时间默认是7200s，每隔一段时间默认是75s，连续多少次默认是9次
4. 状态码
   * 1XX：信息提示
     * 100 ：继续
     * 101 ：切换协议
   * 2XX ：成功，表明服务器成功接收了客户请求
     * 200 ：确定，客户端请求成功
     * 201 ：已创建
     * 202 ：已接受
     * 205 ：重置内容
     * **206 ：表示响应返回的body数据并不是资源的全部**，而是其中一部分。**有时候会遇到加载图片时遇到 206，导致图片无法加载成功，需要在 nginx 配置中添加配置：proxy_buffer_size 等**
   * 3XX ：重定向，客户端浏览器必须采取更多的操作来实现请求，如，浏览器可能不得不请求服务器上的不同页面，或通过代理服务器重复该请求
     * 301 ：已永久移动。此请求和之后的所有请求都应该转到指定的URL
     * 302 ：对象已移动。表示临时重定向，说明请求的资源还在，但是暂时需要用另一个URL来访问。
     * 304 ：未修改。不具有跳转的含义，表示资源未修改，重定向已存在的缓存文件，用于缓存控制。**如果不想访问缓存，可以强制刷新浏览器使其访问最新的资源**。
     * 307 ：临时重定向
   * 4XX ：客户端错误。发生错误，客户端似乎有问题。如，客户端请求不存在的页面
     * 400 ：错误的请求。**表示客户端请求的报文有错误，一般用于后端抛出强制校验的时候提示客户端需要补充哪些信息。客户端需要完善报文或修改报文，或直接将错误提示给用户，由用户修改对应的相关信息后再次提交**
     * 401 ：访问被拒绝。**请求要求用户认证，报这个错时需要考虑一下用户是否认证或认证是否过期**
     * 403 ：**服务器拒绝请求。可以理解为没有权限访问此网站，服务器能收到请求但拒绝提供服务。服务器禁止访问资源**
     * 404 ：服务器找不到请求的页面。**前端检查请求的 url 是否正确，或者后端开发完成之后是否发布成功**
   * 5XX ：服务器错误。服务器由于遇到错误而不能完成该请求
     * 500 ： 内部服务器错误。**一般遇到这种情况，通常会检查一下是否有参数漏传或传参格式不正确的问题**。
     * 501 ：页眉值指定了未实现的配置
     * 503 ：目前服务器无法使用，一般是因为服务器超载或停止维护。表示服务器当前很忙，暂时无法响应服务器。类似于“忘了服务正忙，请稍后重试” 的意思
     * 504 ： 网关超时
     * 505 ：HTTP版本不受支持



## Linux

1. 查看某一进程占用的内存

   * 查看某一个进程所占用的内存，首先可以通过 ps 命令找到进程 id

     **ps  -ef  |  grep  nacos**

     可以看到 nacos 进程号为 461（appadmin 后面的数字）

     然后使用 top 命令 查看内存

     **top  -p  461**

     可以动态实时的看到 CPU 和 内存的占用率，然后，**按 q 键回到**命令行

   * 直接使用 ps 命令查看内存情况

     ps  -aux  |  grep  nacos

     结果如下

     ~~~shell
     appadmin  461  9.9 8.6 6977752 1413148 ...
     
     ## 第一个数字是 pid
     ## 第二个数字 和 第三个数字是 CPU 和 内存的占用率
     ## 第五个数字 1413148 是 物理内存的使用量，单位为 K，
     ~~~

   * 查看进程的 status 文件

     cat  /proc/461/status

     VmRSS 对应的值就是物理内存占用

   * free 命令查看总体内存使用情况

     free  -m

     free  -g

2. vi 的使用

   * vi 有三种状态，命令模式、插入模式、底行模式
   * vi + 文件名，进入 命令模式，
   * 在命令模式按下 【i】键进入 插入模式：
     * 按 [ i ]，从光标当前位置开始输入文字
     * 按 [ a ]，从当前光标下一个位置开始输入文字
     * 按 [ o ]，插入新的一行，从行首开始输入文字
   * 插入模式 按下 [ESC] 键，转到命令行模式，
   * 在命令行模式下，按 [ : ] 键进入 底行模式：
     * [ set nu ]，列出行号，文件中的每一行前面列出行号
     * [ 数字 ]，在 冒号后面输入数字，按回车键，跳到该行
     * [ / 关键字]，关键字是你想寻找的字符，如果第一次找到的不是想要的，按 [ n ] 会往后寻找
     * [ ? 关键字]，按 [ n ] 会往前寻找


​     